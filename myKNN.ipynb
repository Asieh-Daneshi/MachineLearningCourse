{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with kNN\n",
    "<p style=font-size:20px;color:rgba(255,255,255,255);> This is the second session of our machine learning journey!</p>\n",
    "<p style=font-size:14px;color:rgba(255,255,255,255);> Here we apply kNN on our the data!</p>\n",
    "<p style=font-size:20px;color:yellow;> Importing required libraries </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# pip install numpy\n",
    "import pandas as pd\n",
    "# pip install pandas\n",
    "import matplotlib.pyplot as plt \n",
    "# pip install matplotlib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# pip install sikit-learn   # for sklearn\n",
    "# pip install preprocessing\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "# pip install imblearn\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ====================================================================================================\n",
    "<p style=font-size:25px;color:yellow;> Download dataset </p>\n",
    "https://archive.ics.uci.edu/dataset/159/magic+gamma+telescope\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=color:yellow;font-size:20xp> Openning the csv file, changing the name of the columns, showing the first five columns </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"magic04.data\")      # reading the dataset\n",
    "# print(df)     # showing the dataset\n",
    "cols = [\"fLength\", \"fWidth\", \"fSize\", \"fConc\", \"fConc1\", \"fAsym\", \"fM3Long\", \"fM3Trans\", \"fAlpha\", \"fDist\", \"class\"]        # name of the columns from the other file called \"magic04.names\"\n",
    "\n",
    "df = pd.read_csv(\"magic04.data\", names=cols)      # reading the dataset again, but this time we are naming the columns\n",
    "# print(df)\n",
    "df = df.loc[:,['fAlpha','class']]\n",
    "cols = [\"fAlpha\", \"class\"]        # name of the columns from the other file called \"magic04.names\"\n",
    "\n",
    "df.head()        # shows the first five rows of df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ====================================================================================================\n",
    "<p style=color:yellow;font-size:20xp;>Changing the character labels into numbers, looking at histograms</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this blick, we want to see which feature (label) can separate data classes better. We plot histograms, and the one/ those\n",
    "# that look more separable, are better for our classification goal\n",
    "# Based on the plots that come below, we see that it seems 'fAlpha' is the best feature for our purpose!\n",
    "df[\"class\"].unique()        # shows all the different elements in the column labled \"class\"; exactly like unique in MATLAB\n",
    "df[\"class\"] = (df[\"class\"] == \"g\").astype(int)      # compares the elements in the \"class\" column with \"g\" and returns \"True\" or \"Flase\". Then converts these binaries into integers 1 and 0\n",
    "\n",
    "for label in cols[:-1]:\n",
    "    plt.hist(df[df[\"class\"]==1][label], color='blue', label='gamma', alpha=0.7, density=True)\n",
    "    plt.hist(df[df[\"class\"]==0][label], color='red', label='hadron', alpha=0.7, density=True)\n",
    "    plt.title(label)\n",
    "    plt.ylabel(\"Probability\")\n",
    "    plt.xlabel(label)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ====================================================================================================\n",
    "# Train, validation, test datasets\n",
    "<p style=color:yellow;font-size:20xp> First 60% as train, from 60% to 80% as validation, the rest as test </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = np.split(df.sample(frac=1, replace=False), [int(0.6*len(df)), int(0.8*len(df))])\n",
    "# Note:\n",
    "# Randomly sample 30% of the rows with replacement and a specific random state\n",
    "# random_sample_with_replace = df.sample(frac=0.3, replace=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ====================================================================================================\n",
    "<p style=color:yellow;font-size:25xp;>Writing a function that gets data, z-scores each column of data, and balances them (either oversample or undersample), in a way that all of them have the same number of elements </p>\n",
    "<p style=color:rgba(0,255,0,255);font-size:25xp;>Balancing is kind of necessary for KNN; otherwise, the larger class can affect our classification! </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a function called 'scale-dataset' for scaling the data\n",
    "def scale_dataset(dataframe, oversample, undersample):\n",
    "    X = dataframe[dataframe.columns[:-1]].values    # '.columns[:-1] refers to all the columns except than the last column\n",
    "    Y = dataframe[dataframe.columns[-1]].values     # '.columns[-1]' refers to the last column, '.values' turns dataframe into matrix\n",
    "    \n",
    "    # here we put 'StandardScaler()' in a separate variable called 'scaler' to make using it easier. Instead of the next two lines, we could write \"StandardScaler().fit_transform(X)\"\n",
    "    scaler = StandardScaler()       # StandardScaler transforms the data in a way that it has zero mean and a standard deviation of 1.   \n",
    "    X = scaler.fit_transform(X)     # fit_transform finds the mean and std of each column of data. Then for all the element of each column, subtracts the mean of them, and divides the result to the std\n",
    "    # fit_transform and StandardScaler together work as z-tansform\n",
    "    if oversample:\n",
    "        # here we put 'RandomOverSampler()' in a separate variable calles 'ros' to make using it easier\n",
    "        ros = RandomOverSampler()   \n",
    "        X, Y = ros.fit_resample(X, Y)       \n",
    "        # 'fit_resample' picks samples from 'X' for each group of 'Y'. In this example Y includes two groups 1, and 0. Now, using 'OverSampler'\n",
    "        # means that if one of these groups has less members than the other group, it randomly picks some elements from that group again (repeat them)\n",
    "        # in a way that at the end both group have the same number of elements (the size of the larger group)\n",
    "\n",
    "    if undersample:\n",
    "        # here we put 'RandomUnderSampler()' in a separate variable calles 'rus' to make using it easier\n",
    "        rus = RandomUnderSampler()   \n",
    "        X, Y = rus.fit_resample(X, Y)       \n",
    "        # 'fit_resample' picks samples from 'X' for each group of 'Y'. In this example Y includes two groups 1, and 0. Now, using 'OverSampler'\n",
    "        # means that if one of these groups has less members than the other group, it randomly removes some elements from larger group in a way\n",
    "        # that at the end both group have the same number of elements (the size of the smaller group)\n",
    "\n",
    "    data = np.hstack((X, np.reshape(Y, (-1,1))))        # '-1' in 'reshape' means that we leave that dimension unassigned, meaning that here the result has one column, but the number of rows is not assigned\n",
    "    # hstack horizontally stacks the input arrays\n",
    "    return data, X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ====================================================================================================\n",
    "<p style=color:yellow;font-size:25px;>Testing the function that we wrote in the previous block</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ====================================================================================================\n",
    "## Using our function to take samples from data for training, validation, and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, Xtrain, Ytrain = scale_dataset(train, oversample=True, undersample=False)\n",
    "valid, Xvalid, Yvalid = scale_dataset(valid, oversample=False, undersample=False)\n",
    "test, Xtest, Ytest = scale_dataset(test, oversample=False, undersample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=color:yellow;font-size:40px;>kNN</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ypred = knn_model.predict(Xtest)\n",
    "# print(Ypred)\n",
    "# print(Ytest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Ytest,Ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=color:rgba(0,255,0,255);font-size:25xp;>Evaluating the accuracy of our classification (just accuracy, not the whole report!)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Ytest, Ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=color:rgba(255,255,0,255);font-size:25xp;>Testing different number of neighbors for KNN to find the best one!</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "Ks = np.arange(3,20,2)\n",
    "best_score = -1\n",
    "best_K = None\n",
    "\n",
    "for k in Ks:\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn_model, Xtrain, Ytrain, cv=5)  # 5-fold cross-validation\n",
    "    average_score = np.mean(scores)\n",
    "\n",
    "    if average_score > best_score:\n",
    "        best_score = average_score\n",
    "        best_K = k\n",
    "\n",
    "print(\"Best score is: \", best_score)\n",
    "print(\"Best number of neighbors: \", best_K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=color:rgba(255,255,0,255);font-size:25xp;>Testing different algorithms for finding the distance between points on KNN to find the best one!</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "best_score = -1\n",
    "best_K = None\n",
    "\n",
    "for k in Ks:\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=3)\n",
    "    scores = cross_val_score(knn_model, Xtrain, Ytrain, cv=5)  # 5-fold cross-validation\n",
    "    average_score = np.mean(scores)\n",
    "\n",
    "    if average_score > best_score:\n",
    "        best_score = average_score\n",
    "        best_K = k\n",
    "\n",
    "print(\"Best score is: \", best_score)\n",
    "print(\"Best number of neighbors: \", best_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = -1\n",
    "best_distance = None\n",
    "knn_model = KNeighborsClassifier(n_neighbors=3, metric='euclidean')\n",
    "scores = cross_val_score(knn_model, Xtrain, Ytrain, cv=5)  # 5-fold cross-validation\n",
    "average_score = np.mean(scores)\n",
    "\n",
    "if average_score > best_score:\n",
    "    best_score = average_score\n",
    "    best_distance = 'euclidean'\n",
    "# ----------------------------------------------------------------------------------\n",
    "knn_model = KNeighborsClassifier(n_neighbors=3, metric='manhattan')\n",
    "scores = cross_val_score(knn_model, Xtrain, Ytrain, cv=5)  # 5-fold cross-validation\n",
    "average_score = np.mean(scores)\n",
    "\n",
    "if average_score > best_score:\n",
    "    best_score = average_score\n",
    "    best_distance = 'manhattan'\n",
    "# ----------------------------------------------------------------------------------\n",
    "knn_model = KNeighborsClassifier(n_neighbors=3, metric='minkowski', p=2)\n",
    "scores = cross_val_score(knn_model, Xtrain, Ytrain, cv=5)  # 5-fold cross-validation\n",
    "average_score = np.mean(scores)\n",
    "\n",
    "if average_score > best_score:\n",
    "    best_score = average_score\n",
    "    best_distance = 'minkowski'\n",
    "# ----------------------------------------------------------------------------------\n",
    "knn_model = KNeighborsClassifier(n_neighbors=3, metric='chebyshev')\n",
    "scores = cross_val_score(knn_model, Xtrain, Ytrain, cv=5)  # 5-fold cross-validation\n",
    "average_score = np.mean(scores)\n",
    "\n",
    "if average_score > best_score:\n",
    "    best_score = average_score\n",
    "    best_distance = 'chebyshev'\n",
    "# ----------------------------------------------------------------------------------\n",
    "# knn_model = KNeighborsClassifier(n_neighbors=11, metric='mahalanobis')\n",
    "# knn_model.fit(Xtrain, Ytrain)\n",
    "# Ypred = knn_model.predict(Xtest)\n",
    "# allAccuracies.append(accuracy_score(Ytest, Ypred))\n",
    "# ----------------------------------------------------------------------------------\n",
    "knn_model = KNeighborsClassifier(n_neighbors=3, metric='hamming')\n",
    "scores = cross_val_score(knn_model, Xtrain, Ytrain, cv=5)  # 5-fold cross-validation\n",
    "average_score = np.mean(scores)\n",
    "\n",
    "if average_score > best_score:\n",
    "    best_score = average_score\n",
    "    best_distance = 'hamming'\n",
    "# ----------------------------------------------------------------------------------\n",
    "knn_model = KNeighborsClassifier(n_neighbors=3, metric='canberra')\n",
    "scores = cross_val_score(knn_model, Xtrain, Ytrain, cv=5)  # 5-fold cross-validation\n",
    "average_score = np.mean(scores)\n",
    "\n",
    "if average_score > best_score:\n",
    "    best_score = average_score\n",
    "    best_distance = 'canberra'\n",
    "# ----------------------------------------------------------------------------------\n",
    "knn_model = KNeighborsClassifier(n_neighbors=3, metric='cosine')\n",
    "scores = cross_val_score(knn_model, Xtrain, Ytrain, cv=5)  # 5-fold cross-validation\n",
    "average_score = np.mean(scores)\n",
    "\n",
    "if average_score > best_score:\n",
    "    best_score = average_score\n",
    "    best_distance = 'cosine'\n",
    "# ----------------------------------------------------------------------------------\n",
    "knn_model = KNeighborsClassifier(n_neighbors=3, metric='jaccard')\n",
    "scores = cross_val_score(knn_model, Xtrain, Ytrain, cv=5)  # 5-fold cross-validation\n",
    "average_score = np.mean(scores)\n",
    "\n",
    "if average_score > best_score:\n",
    "    best_score = average_score\n",
    "    best_distance = 'jaccard'\n",
    "# ----------------------------------------------------------------------------------\n",
    "knn_model = KNeighborsClassifier(n_neighbors=3, metric='braycurtis')\n",
    "scores = cross_val_score(knn_model, Xtrain, Ytrain, cv=5)  # 5-fold cross-validation\n",
    "average_score = np.mean(scores)\n",
    "\n",
    "if average_score > best_score:\n",
    "    best_score = average_score\n",
    "    best_distance = 'braycurtis'\n",
    "# ----------------------------------------------------------------------------------\n",
    "# knn_model = KNeighborsClassifier(n_neighbors=11, metric='haversine')\n",
    "# knn_model.fit(Xtrain, Ytrain)\n",
    "# Ypred = knn_model.predict(Xtest)\n",
    "# allAccuracies.append(accuracy_score(Ytest, Ypred))\n",
    "\n",
    "\n",
    "print(best_distance)\n",
    "print(best_score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
